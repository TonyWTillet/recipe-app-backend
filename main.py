from fastapi import FastAPI
from pydantic import BaseModel
from transformers import MT5Tokenizer, MT5ForConditionalGeneration, AutoTokenizer, AutoModelForCausalLM, AutoTokenizer, AutoModelForCausalLM, BloomTokenizerFast, BloomForCausalLM
import torch

# === üîÅ Charger les mod√®les fine-tun√©s ===
# MT5 model
mt5_model_path = "./mt5-recipes/checkpoint-1000"
mt5_tokenizer = MT5Tokenizer.from_pretrained(mt5_model_path)
mt5_model = MT5ForConditionalGeneration.from_pretrained(mt5_model_path)

# GPT2 model (essayobsmrm8488/gpt2-finetuned-recipes-cooking_v2)
gpt2_tokenizer = AutoTokenizer.from_pretrained("mrm8488/gpt2-finetuned-recipes-cooking_v2")
gpt2_model = AutoModelForCausalLM.from_pretrained("mrm8488/gpt2-finetuned-recipes-cooking_v2")

# BLOOM model
bloom_tokenizer = BloomTokenizerFast.from_pretrained("bigscience/bloom-560m")
bloom_model = BloomForCausalLM.from_pretrained("bigscience/bloom-560m")

# === üåê Contexte pour les id√©es de recettes ===
cooking_context = """Tu es un chef cr√©atif qui propose des id√©es de recettes. Pour chaque demande, donne exactement 3 suggestions de recettes diff√©rentes.
Format de r√©ponse attendu :
1. [Nom de la recette 1] - [Description courte en une phrase]
2. [Nom de la recette 2] - [Description courte en une phrase]
3. [Nom de la recette 3] - [Description courte en une phrase]"""

# === üç≥ G√©n√©rateur d'id√©es de recettes pour MT5 ===
def generate_text_mt5(prompt: str) -> str:
    try:
        prompt_with_context = f"{cooking_context}\n\nDemande : {prompt}\n\nSuggestions :"

        input_ids = mt5_tokenizer.encode(prompt_with_context, return_tensors="pt", truncation=True, max_length=512)

        output_ids = mt5_model.generate(
            input_ids,
            max_length=200,
            num_beams=4,
            no_repeat_ngram_size=2,
            early_stopping=True,
            do_sample=True,  # Activ√© pour permettre l'utilisation de temperature et top_p
            temperature=0.9,
            top_p=0.95,
            repetition_penalty=1.2,
            min_length=50
        )

        response = mt5_tokenizer.decode(output_ids[0], skip_special_tokens=True)
        
        # Nettoyer la r√©ponse
        response = response.replace(prompt_with_context, "").strip()
        
        # V√©rifier si la r√©ponse contient au moins 2 suggestions
        if not response or len(response.split('\n')) < 2:
            raise ValueError("Pas assez de suggestions g√©n√©r√©es.")

        return response

    except Exception as e:
        return f"D√©sol√©, une erreur est survenue : {str(e)}. Veuillez r√©essayer plus tard."

# === üç≥ G√©n√©rateur d'id√©es de recettes pour GPT2 ===
def generate_text_gpt2(prompt: str) -> str:
    try:
        formatted_prompt = f"{cooking_context}\n\nDemande : {prompt}\n\nSuggestions :"
        
        inputs = gpt2_tokenizer(formatted_prompt, return_tensors="pt", truncation=True, max_length=512)
        outputs = gpt2_model.generate(
            **inputs,
            max_length=200,
            num_beams=4,
            no_repeat_ngram_size=2,
            early_stopping=True,
            do_sample=True,  # Activ√© pour permettre l'utilisation de temperature et top_p
            temperature=0.7,
            top_p=0.95,
            repetition_penalty=1.2,
            min_length=50
        )
        
        generated_text = gpt2_tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # Nettoyer la r√©ponse
        generated_text = generated_text.replace(formatted_prompt, "").strip()

        # V√©rifier si la r√©ponse contient au moins 2 suggestions
        if not generated_text or len(generated_text.split('\n')) < 2:
            raise ValueError("Pas assez de suggestions g√©n√©r√©es.")

        return generated_text

    except Exception as e:
        return f"D√©sol√©, une erreur est survenue : {str(e)}. Veuillez r√©essayer plus tard."
    
# === üç≥ G√©n√©rateur d'id√©es de recettes pour BLOOM ===
def generate_text_bloom(prompt: str) -> str:
    try:
        formatted_prompt = f"{cooking_context}\n\nDemande : {prompt}\n\nSuggestions :"
        
        inputs = bloom_tokenizer(formatted_prompt, return_tensors="pt", truncation=True, max_length=512)
        outputs = bloom_model.generate(
            **inputs,
            max_length=500,
            num_beams=4,
            no_repeat_ngram_size=2,
            early_stopping=True,
            do_sample=True,  # Activ√© pour permettre l'utilisation de temperature et top_p
            temperature=0.7,
            top_p=0.95,
            repetition_penalty=1.2,
            min_length=50
        )
        
        generated_text = bloom_tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # Nettoyer la r√©ponse
        generated_text = generated_text.replace(formatted_prompt, "").strip()

        # V√©rifier si la r√©ponse contient au moins 2 suggestions
        if not generated_text or len(generated_text.split('\n')) < 0:
            raise ValueError("Pas assez de suggestions g√©n√©r√©es.")

        return generated_text

    except Exception as e:
        return f"D√©sol√©, une erreur est survenue : {str(e)}. Veuillez r√©essayer plus tard."

# === üöÄ FastAPI ===
app = FastAPI()

class Prompt(BaseModel):
    prompt: str
    model: str = "mt5"  # Param√®tre pour choisir quel mod√®le utiliser

@app.get("/")
def read_root():
    return {"message": "Bienvenue sur le backend IA de suggestions de recettes !"}

@app.post("/generate_recipe")
def generate_recipe(prompt: Prompt):
    if prompt.model == "mt5":
        response = generate_text_mt5(prompt.prompt)
    elif prompt.model == "gpt2":
        response = generate_text_gpt2(prompt.prompt)
    elif prompt.model == "bloom":
        response = generate_text_bloom(prompt.prompt)
    else:
        response = "Mod√®le non reconnu. Veuillez choisir 'mt5', 'gpt2' ou 'bloom'."
    return {"response": response}
